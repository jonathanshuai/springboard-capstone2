{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training With Transfer Learning\n",
    "It's time to train the CNN. We will be using transfer learning - we'll take an existing, pretrained model and replace the fully connected layer. The model we'll be using is resnet50 model\n",
    "\n",
    "We will train this model using mini-batch gradient descent and allow fine-tuning (allowing the bottleneck layers to be updated with each iteration).\n",
    "\n",
    "In this notebook, we use PyTorch to train the model. Let's import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy and pandas for manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To make validation and training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For training diagnostics later\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Necessary torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "# Used to copy weights later\n",
    "import copy\n",
    "\n",
    "# Custom DataLoader class and transforms from dataloader.py\n",
    "import dataloader\n",
    "from dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some constants to use throughout the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define run constants\n",
    "\n",
    "# csv created in preprocessing that where all the images are\n",
    "PATHS_FILE = '../database/cropped/path_labels.csv' \n",
    "# file from raw data that tells all the class names (alphabetized)\n",
    "ITEM_NAMES_FILE = '../database/raw/food-items.txt'\n",
    "\n",
    "SEED = 17               # Seed for train_test_split \n",
    "\n",
    "IMAGE_SIZE = 224        # Size of input images expected by base model\n",
    "BATCH_SIZE = 8          # Size of each batch \n",
    "N_EPOCHS = 80           # Number of epochs to train for\n",
    "LEARNING_RATE = 1e-4    # Initial learning rate\n",
    "STEP_SIZE = 8           # Number of epochs before one step for exponential decay\n",
    "GAMMA = 0.1             # Amount to reduce learning rate by \n",
    "\n",
    "RUN_NAME = \"batch_size-{}n_epochs-{}learning_rate-{}step_size-{}gamma-{}\"\\\n",
    "    .format(BATCH_SIZE, N_EPOCHS, LEARNING_RATE, STEP_SIZE, GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is located in `../database/cropped`. We'll load it using our `DataLoader` class. \n",
    "The `DataLoader` class has a function `DataLoader.get_data()` which returns a generator that returns data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data...\n",
    "# Read in item names \n",
    "with open(ITEM_NAMES_FILE) as f:\n",
    "    item_names = f.read().splitlines()\n",
    "\n",
    "# Count the number of items\n",
    "n_classes = len(item_names)\n",
    "\n",
    "# Make dictionaries to turn string labels into indicies and back\n",
    "label_dict_itos = dict(zip(range(0, n_classes), item_names))\n",
    "label_dict_stoi = dict(zip(item_names, range(0, n_classes)))\n",
    "\n",
    "# Read csv (we made this in the preprocessing step).\n",
    "df = pd.read_csv(PATHS_FILE)\n",
    "\n",
    "# Get file paths from DataFrame.\n",
    "file_paths = df['cropped_path'].values\n",
    "\n",
    "# Get labels as integer indicies\n",
    "labels = df['label'].map(label_dict_stoi).values\n",
    "\n",
    "# Split into test/validation sets \n",
    "(file_paths_train, file_paths_valid, \n",
    "    labels_train, labels_valid)  = train_test_split(\n",
    "                                    file_paths,\n",
    "                                    labels,\n",
    "                                    stratify=labels,\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=SEED)\n",
    "\n",
    "# List transformations (these are defined in dataloader.py)\n",
    "transforms = [\n",
    "    (lambda x: x,                          {}),\n",
    "    (dataloader.apply_blur,                {}),\n",
    "    (dataloader.apply_brightness,          {}),\n",
    "    (dataloader.apply_color_jitter,        {}),\n",
    "    (dataloader.apply_sp_noise,            {}),\n",
    "    (dataloader.apply_gauss_noise,         {}),\n",
    "    (dataloader.apply_affine,              {}),\n",
    "    (lambda img: dataloader.apply_color_jitter(dataloader.apply_affine(img)), {})\n",
    "]\n",
    "\n",
    "# Create data loader (once again, defined in dataloader.py)\n",
    "data_loader_train = DataLoader(file_paths_train, labels_train, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            image_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                            transforms=transforms)\n",
    "\n",
    "data_loader_valid = DataLoader(file_paths_valid, labels_valid, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            image_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                            transforms=[])\n",
    "\n",
    "dataloaders = {'train': data_loader_train, 'valid': data_loader_valid}\n",
    "dataset_sizes = {phase: dataloaders[phase].shape[0] for phase in dataloaders}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to train our model using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    \"\"\" Train a model and return training history and the trained model.\n",
    "    model               (torch.nn.Model): PyTorch model to train.\n",
    "    \n",
    "    criterion    (torch.nn.modules.loss): PyTorch loss function to optimize for.\n",
    "    \n",
    "    optimizer              (torch.optim): PyTorch optimizer to use when optimizing loss.\n",
    "    \n",
    "    scheduler (torch.optim.lr_scheduler): PyTorch scheduler to schedule the learning rate.\n",
    "    \n",
    "    num_epochs                     (int): Number of epochs to train for.\n",
    "    \"\"\"\n",
    "    # Initialize the best weights.\n",
    "    # We will be keeping track of the best model throughout training\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Lists to keep trach of the changes in loss and accuracy\n",
    "    train_loss_record = []\n",
    "    valid_loss_record = []\n",
    "\n",
    "    train_acc_record = []\n",
    "    valid_acc_record = []\n",
    "\n",
    "    # Current epoch loss and accuracy\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "\n",
    "    # Run for num_epochs epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Do training phase and testing phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            # Take a step for the scheduler if we're training\n",
    "            if phase == 'train':\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step(epoch_loss)\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            # Keep track of the loss and accuracy across batches for this epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Get the data from our DataLoader class\n",
    "            for data in dataloaders[phase].get_data():\n",
    "                inputs, labels = data\n",
    "\n",
    "                # Use PyTorch standard [batch_size, channel, height, width] to make tensors\n",
    "                inputs = torch.tensor([[inp[:, :, 0], \n",
    "                                        inp[:, :, 1], \n",
    "                                        inp[:, :, 2]] for inp in inputs])\\\n",
    "                    .type_as(torch.FloatTensor())\n",
    "                \n",
    "                # Make tensors from labels too\n",
    "                labels = torch.tensor(labels).type_as(torch.LongTensor())\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                # Wrap the Tensors in Variables\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # Reset the optimizer's gradient (some optimizers use previous gradients)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Get the outputs (logits) from the model\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1) \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Calculate gradient and perform backpropagation if training\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Keep track of loss and accuracy for this epoch\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Calculate total loss and accuracy over the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = float(running_corrects) / dataset_sizes[phase]\n",
    "\n",
    "            # Print and record the running loss and accuracy\n",
    "            print('{} Loss : {:.4f} Acc : {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss_record.append(epoch_loss)\n",
    "                train_acc_record.append(epoch_acc)\n",
    "            else:\n",
    "                valid_loss_record.append(epoch_loss)\n",
    "                valid_acc_record.append(epoch_acc)\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Every 10 epochs, save a checkpoint model\n",
    "        if (epoch % 10 == 0):\n",
    "            checkpoint_path = './checkpoints/checkpoint' + RUN_NAME + str(epoch) + '.pt'\n",
    "            torch.save(model, checkpoint_path)\n",
    "            print(\"Saved checkpoint: {}\".format(checkpoint_path))\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Return the model along with the training and validation history\n",
    "    return model, train_loss_record, valid_loss_record, train_acc_record, valid_acc_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the architecture of our model. We take a base model and add our own fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransferModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes):\n",
    "        super(TransferModel, self).__init__()\n",
    "        # Remove the fc layer\n",
    "        self.base_layer = nn.Sequential(*list(base_model.children())[:-1])\n",
    "    \n",
    "        # Create our own fully connected layer\n",
    "        self.fc = nn.Linear(base_model.fc.in_features, n_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Connect the bottleneck layers with our fully connected layer \n",
    "        x = self.base_layer(inputs)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create and train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/79\n",
      "----------\n",
      "train Loss : 3.9458 Acc : 0.0289\n",
      "valid Loss : 3.7976 Acc : 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jshuai/anaconda3/envs/food-identifier/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type TransferModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-80learning_rate-0.0001step_size-8gamma-0.10.pt\n",
      "Epoch 1/79\n",
      "----------\n",
      "train Loss : 3.7864 Acc : 0.0764\n",
      "valid Loss : 3.5888 Acc : 0.1240\n",
      "Epoch 2/79\n",
      "----------\n",
      "train Loss : 3.6409 Acc : 0.1136\n",
      "valid Loss : 3.3791 Acc : 0.2107\n",
      "Epoch 3/79\n",
      "----------\n",
      "train Loss : 3.4786 Acc : 0.1622\n",
      "valid Loss : 3.1399 Acc : 0.3678\n",
      "Epoch 4/79\n",
      "----------\n",
      "train Loss : 3.2933 Acc : 0.2676\n",
      "valid Loss : 2.8797 Acc : 0.4339\n",
      "Epoch 5/79\n",
      "----------\n",
      "train Loss : 3.1182 Acc : 0.3543\n",
      "valid Loss : 2.6311 Acc : 0.5124\n",
      "Epoch 6/79\n",
      "----------\n",
      "train Loss : 2.9465 Acc : 0.4081\n",
      "valid Loss : 2.3688 Acc : 0.5744\n",
      "Epoch 7/79\n",
      "----------\n",
      "train Loss : 2.7410 Acc : 0.4669\n",
      "valid Loss : 2.1342 Acc : 0.6488\n",
      "Epoch 8/79\n",
      "----------\n",
      "train Loss : 2.5758 Acc : 0.5186\n",
      "valid Loss : 1.9497 Acc : 0.6694\n",
      "Epoch 9/79\n",
      "----------\n",
      "train Loss : 2.4048 Acc : 0.5610\n",
      "valid Loss : 1.7358 Acc : 0.7149\n",
      "Epoch 10/79\n",
      "----------\n",
      "train Loss : 2.2764 Acc : 0.5713\n",
      "valid Loss : 1.5557 Acc : 0.7397\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-80learning_rate-0.0001step_size-8gamma-0.110.pt\n",
      "Epoch 11/79\n",
      "----------\n",
      "train Loss : 2.1435 Acc : 0.6105\n",
      "valid Loss : 1.4028 Acc : 0.7521\n",
      "Epoch 12/79\n",
      "----------\n",
      "train Loss : 1.9984 Acc : 0.6415\n",
      "valid Loss : 1.3063 Acc : 0.7645\n",
      "Epoch 13/79\n",
      "----------\n",
      "train Loss : 1.8802 Acc : 0.6705\n",
      "valid Loss : 1.2023 Acc : 0.7645\n",
      "Epoch 14/79\n",
      "----------\n",
      "train Loss : 1.8089 Acc : 0.6457\n",
      "valid Loss : 1.0910 Acc : 0.7934\n",
      "Epoch 15/79\n",
      "----------\n",
      "train Loss : 1.6912 Acc : 0.6994\n",
      "valid Loss : 1.0115 Acc : 0.8099\n",
      "Epoch 16/79\n",
      "----------\n",
      "train Loss : 1.6586 Acc : 0.6829\n",
      "valid Loss : 0.9215 Acc : 0.7893\n",
      "Epoch 17/79\n",
      "----------\n",
      "train Loss : 1.5685 Acc : 0.6952\n",
      "valid Loss : 0.8724 Acc : 0.7851\n",
      "Epoch 18/79\n",
      "----------\n",
      "train Loss : 1.4229 Acc : 0.7273\n",
      "valid Loss : 0.8119 Acc : 0.8140\n",
      "Epoch 19/79\n",
      "----------\n",
      "train Loss : 1.3608 Acc : 0.7252\n",
      "valid Loss : 0.7911 Acc : 0.7934\n",
      "Epoch 20/79\n",
      "----------\n",
      "train Loss : 1.3189 Acc : 0.7531\n",
      "valid Loss : 0.7093 Acc : 0.8058\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-80learning_rate-0.0001step_size-8gamma-0.120.pt\n",
      "Epoch 21/79\n",
      "----------\n",
      "train Loss : 1.2817 Acc : 0.7479\n",
      "valid Loss : 0.6775 Acc : 0.8388\n",
      "Epoch 22/79\n",
      "----------\n",
      "train Loss : 1.1616 Acc : 0.7810\n",
      "valid Loss : 0.6780 Acc : 0.8058\n",
      "Epoch 23/79\n",
      "----------\n",
      "train Loss : 1.1364 Acc : 0.7924\n",
      "valid Loss : 0.6404 Acc : 0.8306\n",
      "Epoch 24/79\n",
      "----------\n",
      "train Loss : 1.1349 Acc : 0.7727\n",
      "valid Loss : 0.6092 Acc : 0.8223\n",
      "Epoch 25/79\n",
      "----------\n",
      "train Loss : 1.0816 Acc : 0.7696\n",
      "valid Loss : 0.5926 Acc : 0.8388\n",
      "Epoch 26/79\n",
      "----------\n",
      "train Loss : 1.0161 Acc : 0.7913\n",
      "valid Loss : 0.5779 Acc : 0.8347\n",
      "Epoch 27/79\n",
      "----------\n",
      "train Loss : 1.0164 Acc : 0.7738\n",
      "valid Loss : 0.5548 Acc : 0.8388\n",
      "Epoch 28/79\n",
      "----------\n",
      "train Loss : 1.0164 Acc : 0.7758\n",
      "valid Loss : 0.5348 Acc : 0.8388\n",
      "Epoch 29/79\n",
      "----------\n",
      "train Loss : 0.9080 Acc : 0.8254\n",
      "valid Loss : 0.5104 Acc : 0.8554\n",
      "Epoch 30/79\n",
      "----------\n",
      "train Loss : 0.9039 Acc : 0.8120\n",
      "valid Loss : 0.5321 Acc : 0.8430\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-80learning_rate-0.0001step_size-8gamma-0.130.pt\n",
      "Epoch 31/79\n",
      "----------\n",
      "train Loss : 0.8658 Acc : 0.8161\n",
      "valid Loss : 0.4760 Acc : 0.8760\n",
      "Epoch 32/79\n",
      "----------\n",
      "train Loss : 0.8522 Acc : 0.8233\n",
      "valid Loss : 0.4752 Acc : 0.8554\n",
      "Epoch 33/79\n",
      "----------\n",
      "train Loss : 0.8336 Acc : 0.8275\n",
      "valid Loss : 0.4675 Acc : 0.8595\n",
      "Epoch 34/79\n",
      "----------\n",
      "train Loss : 0.8186 Acc : 0.8254\n",
      "valid Loss : 0.4580 Acc : 0.8760\n",
      "Epoch 35/79\n",
      "----------\n",
      "train Loss : 0.7581 Acc : 0.8316\n",
      "valid Loss : 0.4717 Acc : 0.8430\n",
      "Epoch 36/79\n",
      "----------\n",
      "train Loss : 0.7607 Acc : 0.8440\n",
      "valid Loss : 0.4548 Acc : 0.8636\n",
      "Epoch 37/79\n",
      "----------\n",
      "train Loss : 0.7156 Acc : 0.8461\n",
      "valid Loss : 0.4311 Acc : 0.8760\n",
      "Epoch 38/79\n",
      "----------\n",
      "train Loss : 0.7087 Acc : 0.8440\n",
      "valid Loss : 0.4271 Acc : 0.8636\n",
      "Epoch 39/79\n",
      "----------\n",
      "train Loss : 0.6857 Acc : 0.8574\n",
      "valid Loss : 0.4200 Acc : 0.8802\n",
      "Epoch 40/79\n",
      "----------\n",
      "train Loss : 0.6788 Acc : 0.8461\n",
      "valid Loss : 0.4548 Acc : 0.8719\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-80learning_rate-0.0001step_size-8gamma-0.140.pt\n",
      "Epoch 41/79\n",
      "----------\n",
      "train Loss : 0.6502 Acc : 0.8667\n",
      "valid Loss : 0.4388 Acc : 0.8802\n",
      "Epoch 42/79\n",
      "----------\n",
      "train Loss : 0.6296 Acc : 0.8657\n",
      "valid Loss : 0.4193 Acc : 0.8802\n",
      "Epoch 43/79\n",
      "----------\n",
      "train Loss : 0.6532 Acc : 0.8461\n",
      "valid Loss : 0.4268 Acc : 0.8760\n",
      "Epoch 44/79\n",
      "----------\n",
      "train Loss : 0.5843 Acc : 0.8915\n",
      "valid Loss : 0.4105 Acc : 0.8802\n",
      "Epoch 45/79\n",
      "----------\n",
      "train Loss : 0.5678 Acc : 0.8843\n",
      "valid Loss : 0.3773 Acc : 0.8967\n",
      "Epoch 46/79\n",
      "----------\n",
      "train Loss : 0.5875 Acc : 0.8833\n",
      "valid Loss : 0.3909 Acc : 0.8678\n",
      "Epoch 47/79\n",
      "----------\n",
      "train Loss : 0.5800 Acc : 0.8729\n",
      "valid Loss : 0.4018 Acc : 0.8760\n",
      "Epoch 48/79\n",
      "----------\n",
      "train Loss : 0.5666 Acc : 0.8822\n",
      "valid Loss : 0.3800 Acc : 0.8802\n",
      "Epoch 49/79\n",
      "----------\n",
      "train Loss : 0.5141 Acc : 0.8936\n",
      "valid Loss : 0.3513 Acc : 0.8967\n",
      "Epoch 50/79\n",
      "----------\n",
      "train Loss : 0.5151 Acc : 0.8915\n",
      "valid Loss : 0.3759 Acc : 0.8967\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-80learning_rate-0.0001step_size-8gamma-0.150.pt\n",
      "Epoch 51/79\n",
      "----------\n",
      "train Loss : 0.5302 Acc : 0.8936\n",
      "valid Loss : 0.3404 Acc : 0.8926\n",
      "Epoch 52/79\n",
      "----------\n",
      "train Loss : 0.5209 Acc : 0.8905\n",
      "valid Loss : 0.3659 Acc : 0.8843\n",
      "Epoch 53/79\n",
      "----------\n",
      "train Loss : 0.4931 Acc : 0.9091\n",
      "valid Loss : 0.3825 Acc : 0.8843\n",
      "Epoch 54/79\n",
      "----------\n",
      "train Loss : 0.4492 Acc : 0.9236\n",
      "valid Loss : 0.3753 Acc : 0.8926\n",
      "Epoch 55/79\n",
      "----------\n",
      "train Loss : 0.4260 Acc : 0.9184\n",
      "valid Loss : 0.3694 Acc : 0.8843\n",
      "Epoch 56/79\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Create a transfer learning model with resnet as the base\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "transfer_model = TransferModel(resnet_model, n_classes)\n",
    "\n",
    "# Use cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer; initialize learning rate to LEARNING_RATE\n",
    "optimizer_conv = optim.SGD(transfer_model.parameters(), lr=LEARNING_RATE, \n",
    "                                momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# Decrease learning rate by GAMMA for every STEP_SIZE steps\n",
    "scheduler =  lr_scheduler.StepLR(optimizer_conv, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "# Call the training function\n",
    "(transfer_model, train_loss_record, valid_loss_record, \n",
    "    train_acc_record, valid_acc_record) = train_model(transfer_model, \n",
    "                        criterion, optimizer_conv, scheduler, num_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# result_df[result_df['label'] == 'fish'][['label', 'guessed']]\n",
    "# result_df[result_df['label'] == 'pinto_beans'][['label', 'guessed']]\n",
    "# result_df[result_df['label'] == 'parmesan_cheese'][['label', 'guessed']]\n",
    "\n",
    "# def show_result(result_df, item):\n",
    "#     for i, row in result_df[result_df['label'] == item].iterrows():\n",
    "#         imshow(row['image'], title=\"Label: {}, Guessed: {}\".format(row['label'], row['guessed']), pause=2.5)\n",
    "\n",
    "# for item in group_accuracy.index[:8]:\n",
    "#     show_result(result_df, item)\n",
    "\n",
    "\n",
    "# for _, row in result_df.iterrows():\n",
    "#     if not row['correct']:\n",
    "#         imshow(row['image'], \n",
    "#             title=\"Label: {}, Guessed: {}\".format(row['label'], row['guessed']), pause=2.5) \n",
    "\n",
    "\n",
    "# show_result(result_df, 'beef')\n",
    "# show_result(result_df, 'pork')\n",
    "# show_result(result_df, 'brown_onion')\n",
    "# show_result(result_df, 'chicken_leg')\n",
    "# show_result(result_df, 'mushroom')\n",
    "# show_result(result_df, 'cilantro')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-identifier",
   "language": "python",
   "name": "food-identifier"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
