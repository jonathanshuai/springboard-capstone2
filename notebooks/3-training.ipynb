{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training With Transfer Learning\n",
    "It's time to train the CNN. We will be using transfer learning - we'll take an existing, pretrained model and replace the fully connected layer. The model we'll be using is resnet50 model\n",
    "\n",
    "We will train this model using mini-batch gradient descent and allow fine-tuning (allowing the bottleneck layers to be updated with each iteration).\n",
    "\n",
    "In this notebook, we use PyTorch to train the model. Let's import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy and pandas for manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To make validation and training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For training diagnostics later\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Necessary torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "# Used to copy weights later\n",
    "import copy\n",
    "\n",
    "# Custom DataLoader class and transforms from dataloader.py\n",
    "import dataloader\n",
    "from dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some constants to use throughout the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define run constants\n",
    "\n",
    "# csv created in preprocessing that where all the images are\n",
    "PATHS_FILE = '../database/cropped/path_labels.csv' \n",
    "# file from raw data that tells all the class names (alphabetized)\n",
    "ITEM_NAMES_FILE = '../database/raw/food-items.txt'\n",
    "\n",
    "SEED = 17               # Seed for train_test_split \n",
    "\n",
    "IMAGE_SIZE = 224        # Size of input images expected by base model\n",
    "BATCH_SIZE = 12         # Size of each batch \n",
    "N_EPOCHS = 80           # Number of epochs to train for\n",
    "LEARNING_RATE = 1e-4    # Initial learning rate\n",
    "STEP_SIZE = 8           # Number of epochs before one step for exponential decay\n",
    "GAMMA = 0.1             # Amount to reduce learning rate by \n",
    "\n",
    "RUN_NAME = \"batch_size-{}n_epochs-{}learning_rate-{}step_size-{}gamma-{}\"\\\n",
    "    .format(BATCH_SIZE, N_EPOCHS, LEARNING_RATE, STEP_SIZE, GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is located in `../database/cropped`. We'll load it using our `DataLoader` class. \n",
    "The `DataLoader` class has a function `DataLoader.get_data()` which returns a generator that returns data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data...\n",
    "# Read in item names \n",
    "with open(ITEM_NAMES_FILE) as f:\n",
    "    item_names = f.read().splitlines()\n",
    "\n",
    "# Count the number of items\n",
    "n_classes = len(item_names)\n",
    "\n",
    "# Make dictionaries to turn string labels into indicies and back\n",
    "label_dict_itos = dict(zip(range(0, n_classes), item_names))\n",
    "label_dict_stoi = dict(zip(item_names, range(0, n_classes)))\n",
    "\n",
    "# Read csv (we made this in the preprocessing step).\n",
    "df = pd.read_csv(PATHS_FILE)\n",
    "\n",
    "# Get file paths from DataFrame.\n",
    "file_paths = df['cropped_path'].values\n",
    "\n",
    "# Get labels as integer indicies\n",
    "labels = df['label'].map(label_dict_stoi).values\n",
    "\n",
    "# Split into test/validation sets \n",
    "(file_paths_train, file_paths_valid, \n",
    "    labels_train, labels_valid)  = train_test_split(\n",
    "                                    file_paths,\n",
    "                                    labels,\n",
    "                                    stratify=labels,\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=SEED)\n",
    "\n",
    "# List transformations (these are defined in dataloader.py)\n",
    "transforms = [\n",
    "    (lambda x: x,                          {}),\n",
    "    (dataloader.apply_blur,                {}),\n",
    "    (dataloader.apply_brightness,          {}),\n",
    "    (dataloader.apply_color_jitter,        {}),\n",
    "    (dataloader.apply_sp_noise,            {}),\n",
    "    (dataloader.apply_gauss_noise,         {}),\n",
    "    (dataloader.apply_affine,              {}),\n",
    "    (lambda img: dataloader.apply_color_jitter(dataloader.apply_affine(img)), {})\n",
    "]\n",
    "\n",
    "# Create data loader (once again, defined in dataloader.py)\n",
    "data_loader_train = DataLoader(file_paths_train, labels_train, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            image_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                            transforms=transforms)\n",
    "\n",
    "data_loader_valid = DataLoader(file_paths_valid, labels_valid, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            image_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                            transforms=[])\n",
    "\n",
    "dataloaders = {'train': data_loader_train, 'valid': data_loader_valid}\n",
    "dataset_sizes = {phase: dataloaders[phase].shape[0] for phase in dataloaders}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to train our model using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    \"\"\" Train a model and return training history and the trained model.\n",
    "    model               (torch.nn.Model): PyTorch model to train.\n",
    "    \n",
    "    criterion    (torch.nn.modules.loss): PyTorch loss function to optimize for.\n",
    "    \n",
    "    optimizer              (torch.optim): PyTorch optimizer to use when optimizing loss.\n",
    "    \n",
    "    scheduler (torch.optim.lr_scheduler): PyTorch scheduler to schedule the learning rate.\n",
    "    \n",
    "    num_epochs                     (int): Number of epochs to train for.\n",
    "    \"\"\"\n",
    "    # Initialize the best weights.\n",
    "    # We will be keeping track of the best model throughout training\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Lists to keep trach of the changes in loss and accuracy\n",
    "    train_loss_record = []\n",
    "    valid_loss_record = []\n",
    "\n",
    "    train_acc_record = []\n",
    "    valid_acc_record = []\n",
    "\n",
    "    # Current epoch loss and accuracy\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "\n",
    "    # Run for num_epochs epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Do training phase and testing phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            # Take a step for the scheduler if we're training\n",
    "            if phase == 'train':\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step(epoch_loss)\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            # Keep track of the loss and accuracy across batches for this epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Get the data from our DataLoader class\n",
    "            for data in dataloaders[phase].get_data():\n",
    "                inputs, labels = data\n",
    "\n",
    "                # Use PyTorch standard [batch_size, channel, height, width] to make tensors\n",
    "                inputs = torch.tensor([[inp[:, :, 0], \n",
    "                                        inp[:, :, 1], \n",
    "                                        inp[:, :, 2]] for inp in inputs])\\\n",
    "                    .type_as(torch.FloatTensor())\n",
    "                \n",
    "                # Make tensors from labels too\n",
    "                labels = torch.tensor(labels).type_as(torch.LongTensor())\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                # Wrap the Tensors in Variables\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # Reset the optimizer's gradient (some optimizers use previous gradients)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Get the outputs (logits) from the model\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1) \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Calculate gradient and perform backpropagation if training\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Keep track of loss and accuracy for this epoch\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Calculate total loss and accuracy over the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = float(running_corrects) / dataset_sizes[phase]\n",
    "\n",
    "            # Print and record the running loss and accuracy\n",
    "            print('{} Loss : {:.4f} Acc : {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss_record.append(epoch_loss)\n",
    "                train_acc_record.append(epoch_acc)\n",
    "            else:\n",
    "                valid_loss_record.append(epoch_loss)\n",
    "                valid_acc_record.append(epoch_acc)\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Every 10 epochs, save a checkpoint model\n",
    "        if (epoch % 10 == 0):\n",
    "            checkpoint_path = './checkpoints/checkpoint' + RUN_NAME + str(epoch) + '.pt'\n",
    "            torch.save(model, checkpoint_path)\n",
    "            print(\"Saved checkpoint: {}\".format(checkpoint_path))\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Return the model along with the training and validation history\n",
    "    return model, train_loss_record, valid_loss_record, train_acc_record, valid_acc_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the architecture of our model. We take a base model and add our own fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransferModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes):\n",
    "        super(TransferModel, self).__init__()\n",
    "        # Remove the fc layer\n",
    "        self.base_layer = nn.Sequential(*list(base_model.children())[:-1])\n",
    "    \n",
    "        # Create our own fully connected layer\n",
    "        self.fc = nn.Linear(base_model.fc.in_features, n_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Connect the bottleneck layers with our fully connected layer \n",
    "        x = self.base_layer(inputs)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create and train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/79\n",
      "----------\n",
      "train Loss : 3.9719 Acc : 0.0269\n",
      "valid Loss : 3.8400 Acc : 0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jshuai/anaconda3/envs/food-identifier/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type TransferModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-12n_epochs-80learning_rate-0.0001step_size-8gamma-0.10.pt\n",
      "Epoch 1/79\n",
      "----------\n",
      "train Loss : 3.8254 Acc : 0.0795\n",
      "valid Loss : 3.6788 Acc : 0.1074\n",
      "Epoch 2/79\n",
      "----------\n",
      "train Loss : 3.6905 Acc : 0.1136\n",
      "valid Loss : 3.5089 Acc : 0.1653\n",
      "Epoch 3/79\n",
      "----------\n",
      "train Loss : 3.5686 Acc : 0.1674\n",
      "valid Loss : 3.3297 Acc : 0.2438\n",
      "Epoch 4/79\n",
      "----------\n",
      "train Loss : 3.4326 Acc : 0.2366\n",
      "valid Loss : 3.1378 Acc : 0.2975\n",
      "Epoch 5/79\n",
      "----------\n",
      "train Loss : 3.2828 Acc : 0.2800\n",
      "valid Loss : 2.9411 Acc : 0.4050\n",
      "Epoch 6/79\n",
      "----------\n",
      "train Loss : 3.1364 Acc : 0.3512\n",
      "valid Loss : 2.7463 Acc : 0.4463\n",
      "Epoch 7/79\n",
      "----------\n",
      "train Loss : 2.9803 Acc : 0.4339\n",
      "valid Loss : 2.5575 Acc : 0.5207\n",
      "Epoch 8/79\n",
      "----------\n",
      "train Loss : 2.8286 Acc : 0.4576\n",
      "valid Loss : 2.3493 Acc : 0.5744\n",
      "Epoch 9/79\n",
      "----------\n",
      "train Loss : 2.6879 Acc : 0.5155\n",
      "valid Loss : 2.1900 Acc : 0.6116\n",
      "Epoch 10/79\n",
      "----------\n",
      "train Loss : 2.5798 Acc : 0.5341\n",
      "valid Loss : 2.0248 Acc : 0.6488\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-12n_epochs-80learning_rate-0.0001step_size-8gamma-0.110.pt\n",
      "Epoch 11/79\n",
      "----------\n",
      "train Loss : 2.4182 Acc : 0.5888\n",
      "valid Loss : 1.8711 Acc : 0.6901\n",
      "Epoch 12/79\n",
      "----------\n",
      "train Loss : 2.2835 Acc : 0.6271\n",
      "valid Loss : 1.7371 Acc : 0.7273\n",
      "Epoch 13/79\n",
      "----------\n",
      "train Loss : 2.1746 Acc : 0.6343\n",
      "valid Loss : 1.6158 Acc : 0.7190\n",
      "Epoch 14/79\n",
      "----------\n",
      "train Loss : 2.0223 Acc : 0.6653\n",
      "valid Loss : 1.4824 Acc : 0.7355\n",
      "Epoch 15/79\n",
      "----------\n",
      "train Loss : 1.9835 Acc : 0.6477\n",
      "valid Loss : 1.3851 Acc : 0.7686\n",
      "Epoch 16/79\n",
      "----------\n",
      "train Loss : 1.8444 Acc : 0.6746\n",
      "valid Loss : 1.2976 Acc : 0.7851\n",
      "Epoch 17/79\n",
      "----------\n",
      "train Loss : 1.7556 Acc : 0.6963\n",
      "valid Loss : 1.2191 Acc : 0.7769\n",
      "Epoch 18/79\n",
      "----------\n",
      "train Loss : 1.6764 Acc : 0.7056\n",
      "valid Loss : 1.1225 Acc : 0.7934\n",
      "Epoch 19/79\n",
      "----------\n",
      "train Loss : 1.6048 Acc : 0.7386\n",
      "valid Loss : 1.0560 Acc : 0.7934\n",
      "Epoch 20/79\n",
      "----------\n",
      "train Loss : 1.5375 Acc : 0.7293\n",
      "valid Loss : 0.9957 Acc : 0.7975\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-12n_epochs-80learning_rate-0.0001step_size-8gamma-0.120.pt\n",
      "Epoch 21/79\n",
      "----------\n",
      "train Loss : 1.4621 Acc : 0.7428\n",
      "valid Loss : 0.9476 Acc : 0.8099\n",
      "Epoch 22/79\n",
      "----------\n",
      "train Loss : 1.3955 Acc : 0.7510\n",
      "valid Loss : 0.9121 Acc : 0.8099\n",
      "Epoch 23/79\n",
      "----------\n",
      "train Loss : 1.3348 Acc : 0.7634\n",
      "valid Loss : 0.8785 Acc : 0.8017\n",
      "Epoch 24/79\n",
      "----------\n",
      "train Loss : 1.3067 Acc : 0.7800\n",
      "valid Loss : 0.8345 Acc : 0.8099\n",
      "Epoch 25/79\n",
      "----------\n",
      "train Loss : 1.2427 Acc : 0.7665\n",
      "valid Loss : 0.7578 Acc : 0.8430\n",
      "Epoch 26/79\n",
      "----------\n",
      "train Loss : 1.2392 Acc : 0.7665\n",
      "valid Loss : 0.7531 Acc : 0.8264\n",
      "Epoch 27/79\n",
      "----------\n",
      "train Loss : 1.1482 Acc : 0.7913\n",
      "valid Loss : 0.7410 Acc : 0.8140\n",
      "Epoch 28/79\n",
      "----------\n",
      "train Loss : 1.0935 Acc : 0.7996\n",
      "valid Loss : 0.7327 Acc : 0.8223\n",
      "Epoch 29/79\n",
      "----------\n",
      "train Loss : 1.0820 Acc : 0.7955\n",
      "valid Loss : 0.6710 Acc : 0.8512\n",
      "Epoch 30/79\n",
      "----------\n",
      "train Loss : 1.0213 Acc : 0.8048\n",
      "valid Loss : 0.6303 Acc : 0.8471\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-12n_epochs-80learning_rate-0.0001step_size-8gamma-0.130.pt\n",
      "Epoch 31/79\n",
      "----------\n",
      "train Loss : 1.0044 Acc : 0.8089\n",
      "valid Loss : 0.6226 Acc : 0.8512\n",
      "Epoch 32/79\n",
      "----------\n",
      "train Loss : 0.9670 Acc : 0.8171\n",
      "valid Loss : 0.6071 Acc : 0.8512\n",
      "Epoch 33/79\n",
      "----------\n",
      "train Loss : 0.9438 Acc : 0.8171\n",
      "valid Loss : 0.6175 Acc : 0.8430\n",
      "Epoch 34/79\n",
      "----------\n",
      "train Loss : 0.9263 Acc : 0.8037\n",
      "valid Loss : 0.5713 Acc : 0.8554\n",
      "Epoch 35/79\n",
      "----------\n",
      "train Loss : 0.8322 Acc : 0.8388\n",
      "valid Loss : 0.5653 Acc : 0.8430\n",
      "Epoch 36/79\n",
      "----------\n",
      "train Loss : 0.8592 Acc : 0.8347\n",
      "valid Loss : 0.5469 Acc : 0.8595\n",
      "Epoch 37/79\n",
      "----------\n",
      "train Loss : 0.8378 Acc : 0.8368\n",
      "valid Loss : 0.5276 Acc : 0.8512\n",
      "Epoch 38/79\n",
      "----------\n",
      "train Loss : 0.7716 Acc : 0.8533\n",
      "valid Loss : 0.5154 Acc : 0.8595\n",
      "Epoch 39/79\n",
      "----------\n",
      "train Loss : 0.7955 Acc : 0.8523\n",
      "valid Loss : 0.5032 Acc : 0.8595\n",
      "Epoch 40/79\n",
      "----------\n",
      "train Loss : 0.7887 Acc : 0.8357\n",
      "valid Loss : 0.5084 Acc : 0.8595\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-12n_epochs-80learning_rate-0.0001step_size-8gamma-0.140.pt\n",
      "Epoch 41/79\n",
      "----------\n",
      "train Loss : 0.7173 Acc : 0.8512\n",
      "valid Loss : 0.5061 Acc : 0.8595\n",
      "Epoch 42/79\n",
      "----------\n",
      "train Loss : 0.7096 Acc : 0.8492\n",
      "valid Loss : 0.4870 Acc : 0.8430\n",
      "Epoch 43/79\n",
      "----------\n",
      "train Loss : 0.7327 Acc : 0.8461\n",
      "valid Loss : 0.4610 Acc : 0.8636\n",
      "Epoch 44/79\n",
      "----------\n",
      "train Loss : 0.7178 Acc : 0.8409\n",
      "valid Loss : 0.4719 Acc : 0.8512\n",
      "Epoch 45/79\n",
      "----------\n",
      "train Loss : 0.6706 Acc : 0.8626\n",
      "valid Loss : 0.4450 Acc : 0.8554\n",
      "Epoch 46/79\n",
      "----------\n",
      "train Loss : 0.6460 Acc : 0.8791\n",
      "valid Loss : 0.4530 Acc : 0.8636\n",
      "Epoch 47/79\n",
      "----------\n",
      "train Loss : 0.6539 Acc : 0.8626\n",
      "valid Loss : 0.4369 Acc : 0.8554\n",
      "Epoch 48/79\n",
      "----------\n",
      "train Loss : 0.6037 Acc : 0.8812\n",
      "valid Loss : 0.4533 Acc : 0.8595\n",
      "Epoch 49/79\n",
      "----------\n",
      "train Loss : 0.6436 Acc : 0.8605\n",
      "valid Loss : 0.4280 Acc : 0.8554\n",
      "Epoch 50/79\n",
      "----------\n",
      "train Loss : 0.6178 Acc : 0.8740\n",
      "valid Loss : 0.4500 Acc : 0.8595\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-12n_epochs-80learning_rate-0.0001step_size-8gamma-0.150.pt\n",
      "Epoch 51/79\n",
      "----------\n",
      "train Loss : 0.5873 Acc : 0.8812\n",
      "valid Loss : 0.4449 Acc : 0.8678\n",
      "Epoch 52/79\n",
      "----------\n",
      "train Loss : 0.5859 Acc : 0.8853\n",
      "valid Loss : 0.4175 Acc : 0.8760\n",
      "Epoch 53/79\n",
      "----------\n",
      "train Loss : 0.5449 Acc : 0.8936\n",
      "valid Loss : 0.4221 Acc : 0.8636\n",
      "Epoch 54/79\n",
      "----------\n",
      "train Loss : 0.5372 Acc : 0.8874\n",
      "valid Loss : 0.4337 Acc : 0.8595\n",
      "Epoch 55/79\n",
      "----------\n",
      "train Loss : 0.5403 Acc : 0.8822\n",
      "valid Loss : 0.4075 Acc : 0.8678\n",
      "Epoch 56/79\n",
      "----------\n",
      "train Loss : 0.5387 Acc : 0.8884\n",
      "valid Loss : 0.4265 Acc : 0.8678\n",
      "Epoch 57/79\n",
      "----------\n",
      "train Loss : 0.4994 Acc : 0.9029\n",
      "valid Loss : 0.4022 Acc : 0.8719\n",
      "Epoch 58/79\n",
      "----------\n",
      "train Loss : 0.5005 Acc : 0.8977\n",
      "valid Loss : 0.4025 Acc : 0.8719\n",
      "Epoch 59/79\n",
      "----------\n",
      "train Loss : 0.5020 Acc : 0.8988\n",
      "valid Loss : 0.3799 Acc : 0.8884\n",
      "Epoch 60/79\n",
      "----------\n",
      "train Loss : 0.4767 Acc : 0.9008\n",
      "valid Loss : 0.3772 Acc : 0.8802\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-12n_epochs-80learning_rate-0.0001step_size-8gamma-0.160.pt\n",
      "Epoch 61/79\n",
      "----------\n",
      "train Loss : 0.4737 Acc : 0.8988\n",
      "valid Loss : 0.3843 Acc : 0.8719\n",
      "Epoch 62/79\n",
      "----------\n",
      "train Loss : 0.4850 Acc : 0.8936\n",
      "valid Loss : 0.3618 Acc : 0.8843\n",
      "Epoch 63/79\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "## Create a transfer learning model with resnet as the base\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "transfer_model = TransferModel(resnet_model, n_classes)\n",
    "\n",
    "# Use cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer; initialize learning rate to LEARNING_RATE\n",
    "optimizer_conv = optim.SGD(transfer_model.parameters(), lr=LEARNING_RATE, \n",
    "                                momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# Decrease learning rate by GAMMA for every STEP_SIZE steps\n",
    "scheduler =  lr_scheduler.StepLR(optimizer_conv, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "# Call the training function\n",
    "(transfer_model, train_loss_record, valid_loss_record, \n",
    "    train_acc_record, valid_acc_record) = train_model(transfer_model, \n",
    "                        criterion, optimizer_conv, scheduler, num_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is finally done training. Let's graph the training record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "x = range(N_EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, valid_loss_record, label='valid')\n",
    "plt.plot(x, train_loss_record, label='train')\n",
    "\n",
    "plt.title(\"Training History\", fontsize=24)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy history\n",
    "x = range(N_EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, valid_acc_record, label='valid')\n",
    "plt.plot(x, train_acc_record, label='train')\n",
    "\n",
    "plt.title(\"Training History\", fontsize=24)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best validation accuracy goes up to around ~90.1%. Although it may seem strange that the training loss is often higher than the validation loss, it makes sense considering the transformations we apply on the training set.\n",
    "\n",
    "Let's look at which classes we did well on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results DataFrame so that we can figure out where we did well\n",
    "truth_hist = []\n",
    "preds_hist = []\n",
    "inputs_hist = []\n",
    "for inputs, labels in dataloaders['valid'].get_data():\n",
    "    inputs_hist.extend(inputs)\n",
    "    inputs = torch.tensor([[inp[:, :, 0], \n",
    "                            inp[:, :, 1], \n",
    "                            inp[:, :, 2]] \n",
    "                            for inp in inputs])\\\n",
    "                            .type_as(torch.FloatTensor())\n",
    "\n",
    "    probs = transfer_model(inputs)\n",
    "    preds = np.argmax(probs.data.numpy(), axis=1)\n",
    "    preds_hist.extend(preds)\n",
    "    truth_hist.extend(labels)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df['truth_code'] = truth_hist\n",
    "result_df['preds_code'] = preds_hist\n",
    "result_df['image'] = inputs_hist\n",
    "result_df['correct'] = result_df['truth_code'] == result_df['preds_code']\n",
    "result_df['label'] = result_df['truth_code'].map(label_dict_itos)\n",
    "result_df['guessed'] = result_df['preds_code'].map(label_dict_itos)\n",
    "\n",
    "accuracy = result_df['correct'].mean()\n",
    "group_accuracy = result_df.groupby('label')['correct'].mean().sort_values()\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "group_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the misclassified images, and what the model mistakened them for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "\n",
    "incorrect_df = result_df[result_df['correct'] == False].sample(15).reset_index()\n",
    "for i, row in incorrect_df.iterrows():\n",
    "    plt.subplot(5, 3, i + 1)\n",
    "    plt.imshow(row['image'])\n",
    "    plt.title(\"Guessed: {} | Actually: {}\".format(row['guessed'], row['label']))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# result_df[result_df['label'] == 'fish'][['label', 'guessed']]\n",
    "# result_df[result_df['label'] == 'pinto_beans'][['label', 'guessed']]\n",
    "# result_df[result_df['label'] == 'parmesan_cheese'][['label', 'guessed']]\n",
    "\n",
    "# def show_result(result_df, item):\n",
    "#     for i, row in result_df[result_df['label'] == item].iterrows():\n",
    "#         imshow(row['image'], title=\"Label: {}, Guessed: {}\".format(row['label'], row['guessed']), pause=2.5)\n",
    "\n",
    "# for item in group_accuracy.index[:8]:\n",
    "#     show_result(result_df, item)\n",
    "\n",
    "\n",
    "# for _, row in result_df.iterrows():\n",
    "#     if not row['correct']:\n",
    "#         imshow(row['image'], \n",
    "#             title=\"Label: {}, Guessed: {}\".format(row['label'], row['guessed']), pause=2.5) \n",
    "\n",
    "\n",
    "# show_result(result_df, 'beef')\n",
    "# show_result(result_df, 'pork')\n",
    "# show_result(result_df, 'brown_onion')\n",
    "# show_result(result_df, 'chicken_leg')\n",
    "# show_result(result_df, 'mushroom')\n",
    "# show_result(result_df, 'cilantro')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-identifier",
   "language": "python",
   "name": "food-identifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
