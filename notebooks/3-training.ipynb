{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training With Transfer Learning\n",
    "It's time to train the CNN. We will be using transfer learning - we'll take an existing, pretrained model and replace the fully connected layer. The model we'll be using is resnet50 model\n",
    "\n",
    "We will train this model using mini-batch gradient descent and allow fine-tuning (allowing the bottleneck layers to be updated with each iteration).\n",
    "\n",
    "In this notebook, we use PyTorch to train the model. Let's import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NumPy and pandas for manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To make validation and training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For training diagnostics later\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Necessary torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "# Used to copy weights later\n",
    "import copy\n",
    "\n",
    "# Custom DataLoader class and transforms from dataloader.py\n",
    "import dataloader\n",
    "from dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some constants to use throughout the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define run constants\n",
    "\n",
    "# csv created in preprocessing that where all the images are\n",
    "PATHS_FILE = '../database/cropped/path_labels.csv' \n",
    "# file from raw data that tells all the class names (alphabetized)\n",
    "ITEM_NAMES_FILE = '../database/raw/food-items.txt'\n",
    "\n",
    "SEED = 17               # Seed for train_test_split \n",
    "\n",
    "IMAGE_SIZE = 224        # Size of input images expected by base model\n",
    "BATCH_SIZE = 8          # Size of each batch \n",
    "N_EPOCHS = 100          # Number of epochs to train for\n",
    "LEARNING_RATE = 1e-4    # Initial learning rate\n",
    "STEP_SIZE = 7           # Number of epochs before one step for exponential decay\n",
    "GAMMA = 0.1             # Amount to reduce learning rate by \n",
    "\n",
    "RUN_NAME = \"batch_size-{}n_epochs-{}learning_rate-{}step_size-{}gamma-{}\"\\\n",
    "    .format(BATCH_SIZE, N_EPOCHS, LEARNING_RATE, STEP_SIZE, GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is located in `../database/cropped`. We'll load it using our `DataLoader` class. \n",
    "The `DataLoader` class has a function `DataLoader.get_data()` which returns a generator that returns data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data...\n",
    "# Read in item names \n",
    "with open(ITEM_NAMES_FILE) as f:\n",
    "    item_names = f.read().splitlines()\n",
    "\n",
    "# Count the number of items\n",
    "n_classes = len(item_names)\n",
    "\n",
    "# Make dictionaries to turn string labels into indicies and back\n",
    "label_dict_itos = dict(zip(range(0, n_classes), item_names))\n",
    "label_dict_stoi = dict(zip(item_names, range(0, n_classes)))\n",
    "\n",
    "# Read csv (we made this in the preprocessing step).\n",
    "df = pd.read_csv(PATHS_FILE)\n",
    "\n",
    "# Get file paths from DataFrame.\n",
    "file_paths = df['cropped_path'].values\n",
    "\n",
    "# Get labels as integer indicies\n",
    "labels = df['label'].map(label_dict_stoi).values\n",
    "\n",
    "# Split into test/validation sets \n",
    "(file_paths_train, file_paths_valid, \n",
    "    labels_train, labels_valid)  = train_test_split(\n",
    "                                    file_paths,\n",
    "                                    labels,\n",
    "                                    stratify=labels,\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=SEED)\n",
    "\n",
    "# List transformations (these are defined in dataloader.py)\n",
    "transforms = [\n",
    "#     (lambda x: x,                          {}),\n",
    "    (dataloader.apply_blur,                {}),\n",
    "    (dataloader.apply_brightness,          {}),\n",
    "    (dataloader.apply_color_jitter,        {}),\n",
    "    (dataloader.apply_sp_noise,            {}),\n",
    "    (dataloader.apply_gauss_noise,         {}),\n",
    "    (dataloader.apply_affine,              {}),\n",
    "    (lambda img: dataloader.apply_color_jitter(dataloader.apply_affine(img)), {})\n",
    "]\n",
    "\n",
    "# Create data loader (once again, defined in dataloader.py)\n",
    "data_loader_train = DataLoader(file_paths_train, labels_train, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            image_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                            transforms=transforms)\n",
    "\n",
    "data_loader_valid = DataLoader(file_paths_valid, labels_valid, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            image_size=(IMAGE_SIZE, IMAGE_SIZE), \n",
    "                            transforms=[])\n",
    "\n",
    "dataloaders = {'train': data_loader_train, 'valid': data_loader_valid}\n",
    "dataset_sizes = {phase: dataloaders[phase].shape[0] for phase in dataloaders}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to train our model using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    \"\"\" Train a model and return training history and the trained model.\n",
    "    model               (torch.nn.Model): PyTorch model to train.\n",
    "    \n",
    "    criterion    (torch.nn.modules.loss): PyTorch loss function to optimize for.\n",
    "    \n",
    "    optimizer              (torch.optim): PyTorch optimizer to use when optimizing loss.\n",
    "    \n",
    "    scheduler (torch.optim.lr_scheduler): PyTorch scheduler to schedule the learning rate.\n",
    "    \n",
    "    num_epochs                     (int): Number of epochs to train for.\n",
    "    \"\"\"\n",
    "    # Initialize the best weights.\n",
    "    # We will be keeping track of the best model throughout training\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Lists to keep trach of the changes in loss and accuracy\n",
    "    train_loss_record = []\n",
    "    valid_loss_record = []\n",
    "\n",
    "    train_acc_record = []\n",
    "    valid_acc_record = []\n",
    "\n",
    "    # Current epoch loss and accuracy\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "\n",
    "    # Run for num_epochs epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Do training phase and testing phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            # Take a step for the scheduler if we're training\n",
    "            if phase == 'train':\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step(epoch_loss)\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            # Keep track of the loss and accuracy across batches for this epoch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Get the data from our DataLoader class\n",
    "            for data in dataloaders[phase].get_data():\n",
    "                inputs, labels = data\n",
    "\n",
    "                # Use PyTorch standard [batch_size, channel, height, width] to make tensors\n",
    "                inputs = torch.tensor([[inp[:, :, 0], \n",
    "                                        inp[:, :, 1], \n",
    "                                        inp[:, :, 2]] for inp in inputs])\\\n",
    "                    .type_as(torch.FloatTensor())\n",
    "                \n",
    "                # Make tensors from labels too\n",
    "                labels = torch.tensor(labels).type_as(torch.LongTensor())\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                # Wrap the Tensors in Variables\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # Reset the optimizer's gradient (some optimizers use previous gradients)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Get the outputs (logits) from the model\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1) \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Calculate gradient and perform backpropagation if training\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Keep track of loss and accuracy for this epoch\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Calculate total loss and accuracy over the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = float(running_corrects) / dataset_sizes[phase]\n",
    "\n",
    "            # Print and record the running loss and accuracy\n",
    "            print('{} Loss : {:.4f} Acc : {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss_record.append(epoch_loss)\n",
    "                train_acc_record.append(epoch_acc)\n",
    "            else:\n",
    "                valid_loss_record.append(epoch_loss)\n",
    "                valid_acc_record.append(epoch_acc)\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Every 10 epochs, save a checkpoint model\n",
    "        if (epoch % 10 == 0):\n",
    "            checkpoint_path = './checkpoints/checkpoint' + RUN_NAME + str(epoch) + '.pt'\n",
    "            torch.save(model, checkpoint_path)\n",
    "            print(\"Saved checkpoint: {}\".format(checkpoint_path))\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Return the model along with the training and validation history\n",
    "    return model, train_loss_record, valid_loss_record, train_acc_record, valid_acc_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the architecture of our model. We take a base model and add our own fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransferModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes):\n",
    "        super(TransferModel, self).__init__()\n",
    "        # Remove the fc layer\n",
    "        self.base_layer = nn.Sequential(*list(base_model.children())[:-1])\n",
    "    \n",
    "        # Create our own fully connected layer\n",
    "        self.fc = nn.Linear(base_model.fc.in_features, n_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Connect the bottleneck layers with our fully connected layer \n",
    "        x = self.base_layer(inputs)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create and train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss : 3.9394 Acc : 0.0362\n",
      "valid Loss : 3.7574 Acc : 0.1157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jshuai/anaconda3/envs/food-identifier/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type TransferModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-100learning_rate-0.0001step_size-7gamma-0.10.pt\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss : 3.7876 Acc : 0.0888\n",
      "valid Loss : 3.5674 Acc : 0.1157\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss : 3.6375 Acc : 0.1198\n",
      "valid Loss : 3.3332 Acc : 0.2397\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss : 3.4915 Acc : 0.1911\n",
      "valid Loss : 3.1004 Acc : 0.3471\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss : 3.2957 Acc : 0.2510\n",
      "valid Loss : 2.8398 Acc : 0.4959\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss : 3.1479 Acc : 0.3130\n",
      "valid Loss : 2.5723 Acc : 0.5413\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss : 2.9696 Acc : 0.3771\n",
      "valid Loss : 2.3281 Acc : 0.6033\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss : 2.7886 Acc : 0.4318\n",
      "valid Loss : 2.1067 Acc : 0.6446\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss : 2.6556 Acc : 0.4855\n",
      "valid Loss : 1.9434 Acc : 0.6942\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss : 2.4838 Acc : 0.5052\n",
      "valid Loss : 1.7504 Acc : 0.6818\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss : 2.3321 Acc : 0.5341\n",
      "valid Loss : 1.5703 Acc : 0.7190\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-100learning_rate-0.0001step_size-7gamma-0.110.pt\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss : 2.1624 Acc : 0.6095\n",
      "valid Loss : 1.4349 Acc : 0.7190\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss : 2.0641 Acc : 0.6085\n",
      "valid Loss : 1.3255 Acc : 0.7314\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss : 1.9281 Acc : 0.6591\n",
      "valid Loss : 1.1914 Acc : 0.7603\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss : 1.8670 Acc : 0.6508\n",
      "valid Loss : 1.0903 Acc : 0.7562\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss : 1.7396 Acc : 0.6736\n",
      "valid Loss : 1.0117 Acc : 0.7769\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss : 1.6806 Acc : 0.6756\n",
      "valid Loss : 0.9300 Acc : 0.8058\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss : 1.5865 Acc : 0.6849\n",
      "valid Loss : 0.8718 Acc : 0.8099\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss : 1.5116 Acc : 0.7056\n",
      "valid Loss : 0.8273 Acc : 0.7934\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss : 1.4065 Acc : 0.7159\n",
      "valid Loss : 0.7806 Acc : 0.8058\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss : 1.4463 Acc : 0.6983\n",
      "valid Loss : 0.7783 Acc : 0.7934\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-100learning_rate-0.0001step_size-7gamma-0.120.pt\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss : 1.3367 Acc : 0.7221\n",
      "valid Loss : 0.7174 Acc : 0.8430\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss : 1.2825 Acc : 0.7397\n",
      "valid Loss : 0.6757 Acc : 0.8471\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss : 1.2281 Acc : 0.7366\n",
      "valid Loss : 0.6542 Acc : 0.8388\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss : 1.1948 Acc : 0.7562\n",
      "valid Loss : 0.6292 Acc : 0.8512\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss : 1.1184 Acc : 0.7758\n",
      "valid Loss : 0.6050 Acc : 0.8512\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss : 1.0634 Acc : 0.7893\n",
      "valid Loss : 0.6082 Acc : 0.8595\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss : 1.0402 Acc : 0.7851\n",
      "valid Loss : 0.5973 Acc : 0.8471\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss : 0.9875 Acc : 0.7862\n",
      "valid Loss : 0.5493 Acc : 0.8595\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss : 0.9556 Acc : 0.8027\n",
      "valid Loss : 0.5427 Acc : 0.8554\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss : 1.0181 Acc : 0.7676\n",
      "valid Loss : 0.5358 Acc : 0.8554\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-100learning_rate-0.0001step_size-7gamma-0.130.pt\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss : 0.9294 Acc : 0.7986\n",
      "valid Loss : 0.5050 Acc : 0.8760\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss : 0.8768 Acc : 0.7975\n",
      "valid Loss : 0.4964 Acc : 0.8678\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss : 0.8591 Acc : 0.8151\n",
      "valid Loss : 0.4690 Acc : 0.8636\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss : 0.8941 Acc : 0.8037\n",
      "valid Loss : 0.4660 Acc : 0.8719\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss : 0.8223 Acc : 0.8244\n",
      "valid Loss : 0.4858 Acc : 0.8719\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss : 0.7647 Acc : 0.8378\n",
      "valid Loss : 0.4857 Acc : 0.8719\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss : 0.7684 Acc : 0.8316\n",
      "valid Loss : 0.4826 Acc : 0.8719\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss : 0.7560 Acc : 0.8337\n",
      "valid Loss : 0.4652 Acc : 0.8802\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss : 0.7571 Acc : 0.8182\n",
      "valid Loss : 0.4366 Acc : 0.8802\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss : 0.6781 Acc : 0.8678\n",
      "valid Loss : 0.4486 Acc : 0.8760\n",
      "Saved checkpoint: ./checkpoints/checkpointbatch_size-8n_epochs-100learning_rate-0.0001step_size-7gamma-0.140.pt\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss : 0.7065 Acc : 0.8430\n",
      "valid Loss : 0.4416 Acc : 0.8678\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss : 0.7284 Acc : 0.8295\n",
      "valid Loss : 0.4384 Acc : 0.8884\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss : 0.6437 Acc : 0.8647\n",
      "valid Loss : 0.4275 Acc : 0.8636\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss : 0.6276 Acc : 0.8740\n",
      "valid Loss : 0.4260 Acc : 0.8802\n",
      "Epoch 45/99\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "## Create a transfer learning model with resnet as the base\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "transfer_model = TransferModel(resnet_model, n_classes)\n",
    "\n",
    "# Use cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer; initialize learning rate to LEARNING_RATE\n",
    "optimizer_conv = optim.SGD(transfer_model.parameters(), lr=LEARNING_RATE, \n",
    "                                momentum=0.9, weight_decay=0.001)\n",
    "\n",
    "# Decrease learning rate by GAMMA for every STEP_SIZE steps\n",
    "scheduler =  lr_scheduler.StepLR(optimizer_conv, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "# Call the training function\n",
    "(transfer_model, train_loss_record, valid_loss_record, \n",
    "    train_acc_record, valid_acc_record) = train_model(transfer_model, \n",
    "                        criterion, optimizer_conv, scheduler, num_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is finally done training. Let's graph the training record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss history\n",
    "x = range(N_EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, valid_loss_record, label='valid')\n",
    "plt.plot(x, train_loss_record, label='train')\n",
    "\n",
    "plt.title(\"Training History\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16, labelpad=10)\n",
    "plt.xlabel(\"Epoch\", fontsize=16, labelpad=10)\n",
    "plt.legend()\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy history\n",
    "x = range(N_EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x, valid_acc_record, label='valid')\n",
    "plt.plot(x, train_acc_record, label='train')\n",
    "\n",
    "plt.title(\"Training History\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16, labelpad=10)\n",
    "plt.xlabel(\"Epoch\", fontsize=16, labelpad=10)\n",
    "plt.legend()\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best validation accuracy goes up to around ~90.1%. Although it may seem strange that the training loss is often higher than the validation loss, it makes sense considering the transformations we apply on the training set.\n",
    "\n",
    "Let's look at which classes we did well on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results DataFrame so that we can figure out where we did well\n",
    "truth_hist = []\n",
    "preds_hist = []\n",
    "inputs_hist = []\n",
    "for inputs, labels in dataloaders['valid'].get_data():\n",
    "    inputs_hist.extend(inputs)\n",
    "    inputs = torch.tensor([[inp[:, :, 0], \n",
    "                            inp[:, :, 1], \n",
    "                            inp[:, :, 2]] \n",
    "                            for inp in inputs])\\\n",
    "                            .type_as(torch.FloatTensor())\n",
    "\n",
    "    probs = transfer_model(inputs)\n",
    "    preds = np.argmax(probs.data.numpy(), axis=1)\n",
    "    preds_hist.extend(preds)\n",
    "    truth_hist.extend(labels)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df['truth_code'] = truth_hist\n",
    "result_df['preds_code'] = preds_hist\n",
    "result_df['image'] = inputs_hist\n",
    "result_df['correct'] = result_df['truth_code'] == result_df['preds_code']\n",
    "result_df['label'] = result_df['truth_code'].map(label_dict_itos)\n",
    "result_df['guessed'] = result_df['preds_code'].map(label_dict_itos)\n",
    "\n",
    "accuracy = result_df['correct'].mean()\n",
    "group_accuracy = result_df.groupby('label')['correct'].mean().sort_values()\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "group_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the misclassified images, and what the model mistakened them for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "\n",
    "incorrect_df = result_df[result_df['correct'] == False].sample(15).reset_index()\n",
    "for i, row in incorrect_df.iterrows():\n",
    "    plt.subplot(5, 3, i + 1)\n",
    "    plt.imshow(row['image'])\n",
    "    plt.title(\"Guessed: {} \\n Actually: {}\".format(row['guessed'], row['label']))\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food-identifier",
   "language": "python",
   "name": "food-identifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
